{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 31192,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "digital_marketing_data.csv",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/solomonmawira-dotcom/Diabetes_Patients_Prediction.ipynb/blob/main/digital_marketing_data_csv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-17T14:53:18.736255Z",
          "iopub.execute_input": "2025-11-17T14:53:18.736592Z",
          "iopub.status.idle": "2025-11-17T14:53:18.743318Z",
          "shell.execute_reply.started": "2025-11-17T14:53:18.736564Z",
          "shell.execute_reply": "2025-11-17T14:53:18.74187Z"
        },
        "id": "CdH1EmvlhVRR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Smart Ad Conversion Predictor\n",
        "#AI Engineering Beginner Series_Final Project (Classification)\n",
        "#Purpose:Predict whether a digital ad campaign will convert (binary classification).\n",
        "#This notebook demonstrates full pipeline: EDA, preprocessing, modeling (logistic regression + random forest), evaluation, export for deployment, and pointers for Streamlit deployment.\n",
        "\n",
        "#Dataset:`digital_marketing_data.csv` (expected columns include clicks, impressions, spent, cpc, ctr, platform/ad_type, converted/Converted)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-17T14:53:18.744479Z",
          "iopub.execute_input": "2025-11-17T14:53:18.744775Z",
          "iopub.status.idle": "2025-11-17T14:53:18.763241Z",
          "shell.execute_reply.started": "2025-11-17T14:53:18.744753Z",
          "shell.execute_reply": "2025-11-17T14:53:18.762159Z"
        },
        "id": "ggkT-1GFhVRS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell in Colab to ensure required libraries are present\n",
        "!pip install -q scikit-learn pandas matplotlib seaborn joblib\n",
        "print(\"Installed/checked basic dependencies.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-17T14:53:18.764083Z",
          "iopub.execute_input": "2025-11-17T14:53:18.764306Z",
          "iopub.status.idle": "2025-11-17T14:53:24.81095Z",
          "shell.execute_reply.started": "2025-11-17T14:53:18.764288Z",
          "shell.execute_reply": "2025-11-17T14:53:24.809482Z"
        },
        "id": "qbLCgOk3hVRT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve)\n",
        "\n",
        "sns.set(style=\"whitegrid\", palette=\"muted\")\n",
        "print(\"Imports ready.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-17T14:53:24.814254Z",
          "iopub.execute_input": "2025-11-17T14:53:24.814708Z",
          "iopub.status.idle": "2025-11-17T14:53:26.796908Z",
          "shell.execute_reply.started": "2025-11-17T14:53:24.81465Z",
          "shell.execute_reply": "2025-11-17T14:53:26.795702Z"
        },
        "id": "RBJDISCQhVRT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab-friendly: this code tries Kaggle path, local filename, then opens upload dialog in Colab.\n",
        "DATA_KAGGLE_PATH = '/kaggle/input/predict-conversion-in-digital-marketing-dataset/digital_marketing_data.csv'\n",
        "LOCAL_NAME = 'digital_marketing_data.csv'\n",
        "\n",
        "if os.path.exists(DATA_KAGGLE_PATH):\n",
        "    print(\"Loading dataset from Kaggle path.\")\n",
        "    df = pd.read_csv(DATA_KAGGLE_PATH)\n",
        "elif os.path.exists(LOCAL_NAME):\n",
        "    print(\"Loading dataset from local file:\", LOCAL_NAME)\n",
        "    df = pd.read_csv(LOCAL_NAME)\n",
        "else:\n",
        "    # Try Colab upload\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        print(\"Please upload the dataset file (digital_marketing_data.csv) via the dialog.\")\n",
        "        uploaded = files.upload()\n",
        "        # pick first uploaded file\n",
        "        uploaded_filename = list(uploaded.keys())[0]\n",
        "        df = pd.read_csv(uploaded_filename)\n",
        "    except Exception as e:\n",
        "        raise FileNotFoundError(\"Dataset not found. Place 'digital_marketing_data.csv' in working directory, or upload it in Colab.\")\n",
        "\n",
        "print(\"Loaded dataframe with shape:\", df.shape)\n",
        "df.head()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-17T14:53:26.797749Z",
          "iopub.execute_input": "2025-11-17T14:53:26.798121Z"
        },
        "id": "i5AeVkC8hVRU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic EDA: info, describe, missing values\n",
        "print(\"=== Data Info ===\")\n",
        "display(df.info())\n",
        "\n",
        "print(\"\\n=== Numeric Summary ===\")\n",
        "display(df.describe())\n",
        "\n",
        "print(\"\\n=== Missing values per column ===\")\n",
        "display(df.isnull().sum().sort_values(ascending=False).head(20))\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "GuD_8YdphVRV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Try to detect a target column automatically; common names: Converted, converted, target, is_converted\n",
        "possible_targets = ['converted','Converted','target','is_converted','Converted_flag','converted_flag']\n",
        "target_col = None\n",
        "for c in df.columns:\n",
        "    if c in possible_targets or c.lower() in [p.lower() for p in possible_targets]:\n",
        "        target_col = c\n",
        "        break\n",
        "\n",
        "# fallback: search for binary-like columns\n",
        "if not target_col:\n",
        "    for c in df.columns:\n",
        "        if df[c].dropna().nunique() <= 2:\n",
        "            target_col = c\n",
        "            break\n",
        "\n",
        "print(\"Detected target column:\", target_col)\n",
        "if target_col is not None:\n",
        "    print(df[target_col].value_counts(dropna=False))\n",
        "else:\n",
        "    print(\"No target detected automatically. Edit the notebook to set target_col = 'YourColumnName'\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "7_SKYSrShVRV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- USER ACTION: if target_col detected above is None, set it manually here:\n",
        "# target_col = 'Converted'\n",
        "# -----------------------------------------\n",
        "if 'target_col' not in globals() or target_col is None:\n",
        "    raise ValueError(\"Target column not set. Edit the notebook and set `target_col = 'Converted'` to your dataset's target column name.\")\n",
        "\n",
        "# Work on a copy\n",
        "data = df.copy()\n",
        "\n",
        "# 1) Drop rows where target is missing\n",
        "data = data.dropna(subset=[target_col])\n",
        "\n",
        "# 2) Identify numeric and categorical columns\n",
        "num_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
        "cat_cols = data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "if target_col in num_cols:\n",
        "    num_cols.remove(target_col)\n",
        "\n",
        "print(\"Numerical columns (sample):\", num_cols[:10])\n",
        "print(\"Categorical columns (sample):\", cat_cols[:10])\n",
        "\n",
        "# 3) Drop simple id-like columns\n",
        "id_cols = [c for c in data.columns if 'id' in c.lower() and c.lower()!=target_col.lower()]\n",
        "print(\"Dropping id-like columns:\", id_cols)\n",
        "data = data.drop(columns=id_cols, errors='ignore')\n",
        "\n",
        "# 4) Fill missing numeric with median and categorical with 'Unknown'\n",
        "for c in num_cols:\n",
        "    if data[c].isnull().sum() > 0:\n",
        "        data[c] = data[c].fillna(data[c].median())\n",
        "for c in cat_cols:\n",
        "    if data[c].isnull().sum() > 0:\n",
        "        data[c] = data[c].fillna('Unknown')\n",
        "\n",
        "# 5) Encode categorical features quickly using LabelEncoder (fast for baseline)\n",
        "le = LabelEncoder()\n",
        "for c in cat_cols:\n",
        "    try:\n",
        "        data[c] = le.fit_transform(data[c].astype(str))\n",
        "    except Exception as e:\n",
        "        print(\"Encode error:\", c, e)\n",
        "\n",
        "# 6) Normalize/convert target to 0/1 if needed\n",
        "if set(data[target_col].dropna().unique()) - {0,1}:\n",
        "    if pd.api.types.is_numeric_dtype(data[target_col]):\n",
        "        data[target_col] = (data[target_col] > data[target_col].median()).astype(int)\n",
        "    else:\n",
        "        # map common true-like values\n",
        "        mapping = {}\n",
        "        for val in data[target_col].unique():\n",
        "            sval = str(val).strip().lower()\n",
        "            if sval in ['1','true','yes','converted','y']:\n",
        "                mapping[val] = 1\n",
        "            elif sval in ['0','false','no','n']:\n",
        "                mapping[val] = 0\n",
        "        if mapping:\n",
        "            data[target_col] = data[target_col].map(mapping).fillna(0).astype(int)\n",
        "        else:\n",
        "            data[target_col] = le.fit_transform(data[target_col].astype(str))\n",
        "\n",
        "print(\"Preprocessing complete. Shape:\", data.shape)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "Kj6X3jymhVRW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Features and target\n",
        "X = data.drop(columns=[target_col])\n",
        "y = data[target_col].astype(int)\n",
        "\n",
        "# Split (stratify if possible)\n",
        "if y.nunique() > 1:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "else:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scaling for Logistic Regression\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Logistic Regression baseline\n",
        "lr = LogisticRegression(max_iter=1000)\n",
        "lr.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_lr = lr.predict(X_test_scaled)\n",
        "y_proba_lr = lr.predict_proba(X_test_scaled)[:,1]\n",
        "\n",
        "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
        "print(\"Logistic Regression ROC AUC:\", roc_auc_score(y_test, y_proba_lr))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_lr))\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "jOjP-1e4hVRX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred_lr)\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(\"Confusion Matrix (Logistic Regression)\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()\n",
        "\n",
        "# ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba_lr)\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(fpr, tpr, label=f'LR (AUC = {roc_auc_score(y_test, y_proba_lr):.3f})')\n",
        "plt.plot([0,1], [0,1], '--', color='grey')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve (Logistic Regression)')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "YuVhFHVAhVRY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Random Forest (often better baseline for tabular features)\n",
        "rf = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
        "rf.fit(X_train, y_train)  # RF handles unscaled features well\n",
        "\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "y_proba_rf = rf.predict_proba(X_test)[:,1]\n",
        "\n",
        "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "print(\"Random Forest ROC AUC:\", roc_auc_score(y_test, y_proba_rf))\n",
        "print(\"\\nClassification Report (RF):\\n\", classification_report(y_test, y_pred_rf))\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "GxwQM3TRhVRY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "i4T9gWFqhVRY"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}